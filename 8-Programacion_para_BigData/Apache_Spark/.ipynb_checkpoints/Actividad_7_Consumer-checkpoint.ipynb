{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa127aee",
   "metadata": {},
   "source": [
    "# Information Consumer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "612fd5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# empecemos importando las instancias\n",
    "from pyspark import SparkConf,SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import Row,SQLContext\n",
    "import sys\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ae82224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cuenta cuantas veces aparece un hashtag o lo agrega a la lista\n",
    "def aggregate_tags_count(new_values, total_sum):\n",
    "    return sum(new_values) + (total_sum or 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08b1dc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sql_context_instance(spark_context):\n",
    "    if 'sqlContextSingletonInstance' not in globals():\n",
    "        globals()['sqlContextSingletonInstance'] = SQLContext(spark_context)\n",
    "    return globals()['sqlContextSingletonInstance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58d862ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_rdd(time, rdd):\n",
    "    print(\"----------- %s -----------\" % str(time))\n",
    "    #try:\n",
    "    # obtén el contexto spark sql singleton desde el contexto actual\n",
    "    sql_context = get_sql_context_instance(rdd.context)\n",
    "    print(\"Get spark sql singleton context from the current context ----------- %s -----------\" % str(time))\n",
    "\n",
    "    # convierte el RDD a Row RDD\n",
    "    row_rdd = rdd.map(lambda w: Row(hashtag=w[0], hashtag_count=w[1]))\n",
    "\n",
    "    # crea un DF desde el Row RDD\n",
    "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
    "    print(hashtag_df)\n",
    "\n",
    "    # Registra el DF como tabla\n",
    "    hashtags_df.registerTempTable(\"hashtags\")\n",
    "\n",
    "    # obtenemos la información\n",
    "    hashtag_counts_df = sql_context.sql(\n",
    "        '''SELECT hashtag, hashtag_count FROM hashtags ORDER BY hashtag_count DESC LIMIT 10'''\n",
    "    )# obtén los 10 mejores hashtags de la tabla utilizando SQL e imprímelos\n",
    "    hashtag_counts_df.show()\n",
    "    hashtag_counts_df.coalesce(1)\\\n",
    "        .write.format('com.databricks.spark.csv')\\\n",
    "        .mode('overwrite')\\\n",
    "        .option(\"header\", \"true\")\\\n",
    "        .csv(\"hashtag_file.csv\") \n",
    "\n",
    "    country_counts_df = sql_context.sql(\n",
    "        '''select word as country_code, \n",
    "        word_count as tweet_count from hashtags where word like 'CC%'order by word_count desc limit 10'''\n",
    "    )# obtén los 10 mejores paises de la tabla utilizando SQL e imprímelos\n",
    "    country_counts_df.show()\n",
    "    country_counts_df.coalesce(1)\\\n",
    "        .write.format('com.databricks.spark.csv')\\\n",
    "        .mode('overwrite')\\\n",
    "        .option(\"header\", \"true\")\\\n",
    "        .csv(\"country_file.csv\")\n",
    "    #except:\n",
    "    #    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e415fad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.conf.SparkConf at 0x7f06800f5a90>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A continuación creemos la configuración de spark:\n",
    "conf = SparkConf()\n",
    "conf.setAppName(\"Actividad_7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f79a75ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.1.2-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "21/09/22 23:11:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# creemos el Spark Context\n",
    "sc = SparkContext(conf=conf)\n",
    "sc.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ef45714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos el Streming Context con intervalo de refresco de 5 segundos\n",
    "ssc = StreamingContext(sc, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9195108f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# establecemos un punto de control para la recuperacion de RDD\n",
    "ssc.checkpoint(\"checkpoint_TwitterApp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "078fd9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#por ultimo, leemos los datos:\n",
    "dataStream = ssc.socketTextStream(\"localhost\",9009)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50553ada",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf892430",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:11:26 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:11:26 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:11:27 ERROR JobScheduler: Error running job streaming job 1632352286000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o25.awaitTermination.\n: org.apache.spark.SparkException: An exception was raised by Python:\nTraceback (most recent call last):\n  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n    r = self.func(t, *rdds)\n  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n    hashtags_df = sql_context.createDataFrame(row_rdd)\n  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n    first = rdd.first()\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n    raise ValueError(\"RDD is empty\")\nValueError: RDD is empty\n\n\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_12505/2484403717.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# espera que la transmisión termine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mssc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/streaming/context.py\u001b[0m in \u001b[0;36mawaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[1;32m    198\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jssc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jssc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTerminationOrTimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o25.awaitTermination.\n: org.apache.spark.SparkException: An exception was raised by Python:\nTraceback (most recent call last):\n  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n    r = self.func(t, *rdds)\n  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n    hashtags_df = sql_context.createDataFrame(row_rdd)\n  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n    first = rdd.first()\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n    raise ValueError(\"RDD is empty\")\nValueError: RDD is empty\n\n\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:11:28 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:11:28 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:11:28 ERROR JobScheduler: Error running job streaming job 1632352288000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:11:30 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:11:30 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:11:30 ERROR JobScheduler: Error running job streaming job 1632352290000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:11:32 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:11:32 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:11:32 ERROR JobScheduler: Error running job streaming job 1632352292000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:11:34 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:11:34 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:11:34 ERROR JobScheduler: Error running job streaming job 1632352294000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:11:36 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:11:36 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:11:36 ERROR JobScheduler: Error running job streaming job 1632352296000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:11:38 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:11:38 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:11:38 ERROR JobScheduler: Error running job streaming job 1632352298000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:11:40 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:11:40 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:11:40 ERROR JobScheduler: Error running job streaming job 1632352300000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:11:42 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:11:42 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:11:42 ERROR JobScheduler: Error running job streaming job 1632352302000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:11:44 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:11:44 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:11:44 ERROR JobScheduler: Error running job streaming job 1632352304000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:11:46 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:11:46 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:11:46 ERROR JobScheduler: Error running job streaming job 1632352306000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:11:48 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:11:48 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:11:48 ERROR JobScheduler: Error running job streaming job 1632352308000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:11:50 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:11:50 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:11:50 ERROR JobScheduler: Error running job streaming job 1632352310000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:11:52 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:11:52 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:11:52 ERROR JobScheduler: Error running job streaming job 1632352312000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:11:54 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:11:54 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:11:54 ERROR JobScheduler: Error running job streaming job 1632352314000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:11:56 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:11:56 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:11:56 ERROR JobScheduler: Error running job streaming job 1632352316000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:11:58 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:11:58 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:11:58 ERROR JobScheduler: Error running job streaming job 1632352318000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:12:00 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:12:00 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:12:00 ERROR JobScheduler: Error running job streaming job 1632352320000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:12:02 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:12:02 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:12:02 ERROR JobScheduler: Error running job streaming job 1632352322000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:12:04 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:12:04 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:12:04 ERROR JobScheduler: Error running job streaming job 1632352324000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:12:06 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:12:06 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:12:06 ERROR JobScheduler: Error running job streaming job 1632352326000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:12:08 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:12:08 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:12:08 ERROR JobScheduler: Error running job streaming job 1632352328000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:12:10 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:12:10 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:12:10 ERROR JobScheduler: Error running job streaming job 1632352330000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:12:12 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:12:12 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:12:12 ERROR JobScheduler: Error running job streaming job 1632352332000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:12:14 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:12:14 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:12:14 ERROR JobScheduler: Error running job streaming job 1632352334000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:12:16 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:12:16 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:12:16 ERROR JobScheduler: Error running job streaming job 1632352336000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:12:18 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:12:18 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:12:18 ERROR JobScheduler: Error running job streaming job 1632352338000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:12:20 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:12:20 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:12:20 ERROR JobScheduler: Error running job streaming job 1632352340000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:12:22 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:12:22 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:12:22 ERROR JobScheduler: Error running job streaming job 1632352342000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:12:24 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:12:24 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:12:24 ERROR JobScheduler: Error running job streaming job 1632352344000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:12:26 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:12:26 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:12:26 ERROR JobScheduler: Error running job streaming job 1632352346000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:12:28 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:12:28 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:12:28 ERROR JobScheduler: Error running job streaming job 1632352348000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:12:30 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:12:30 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:12:30 ERROR JobScheduler: Error running job streaming job 1632352350000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:12:32 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:12:32 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:12:32 ERROR JobScheduler: Error running job streaming job 1632352352000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:12:34 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:12:34 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:12:34 ERROR JobScheduler: Error running job streaming job 1632352354000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:12:36 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:12:36 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:12:36 ERROR JobScheduler: Error running job streaming job 1632352356000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:12:38 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:12:38 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:12:38 ERROR JobScheduler: Error running job streaming job 1632352358000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:12:40 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:12:40 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:12:40 ERROR JobScheduler: Error running job streaming job 1632352360000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:12:42 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:12:42 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:12:42 ERROR JobScheduler: Error running job streaming job 1632352362000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:12:44 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:12:44 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:12:44 ERROR JobScheduler: Error running job streaming job 1632352364000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:12:46 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:12:46 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:12:46 ERROR JobScheduler: Error running job streaming job 1632352366000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:12:48 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:12:48 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:12:48 ERROR JobScheduler: Error running job streaming job 1632352368000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:12:50 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:12:50 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:12:50 ERROR JobScheduler: Error running job streaming job 1632352370000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:12:52 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:12:52 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:12:52 ERROR JobScheduler: Error running job streaming job 1632352372000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:12:54 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:12:54 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:12:54 ERROR JobScheduler: Error running job streaming job 1632352374000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:12:56 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:12:56 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:12:56 ERROR JobScheduler: Error running job streaming job 1632352376000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:12:58 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:12:58 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:12:58 ERROR JobScheduler: Error running job streaming job 1632352378000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:13:00 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:13:00 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:13:00 ERROR JobScheduler: Error running job streaming job 1632352380000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:13:02 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:13:02 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:13:02 ERROR JobScheduler: Error running job streaming job 1632352382000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:13:04 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:13:04 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:13:04 ERROR JobScheduler: Error running job streaming job 1632352384000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:13:06 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:13:06 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:13:06 ERROR JobScheduler: Error running job streaming job 1632352386000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:13:08 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:13:08 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:13:08 ERROR JobScheduler: Error running job streaming job 1632352388000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:13:10 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:13:10 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:13:10 ERROR JobScheduler: Error running job streaming job 1632352390000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:13:12 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:13:12 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:13:12 ERROR JobScheduler: Error running job streaming job 1632352392000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:13:14 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:13:14 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:13:14 ERROR JobScheduler: Error running job streaming job 1632352394000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:13:16 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:13:16 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:13:16 ERROR JobScheduler: Error running job streaming job 1632352396000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:13:18 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:13:18 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:13:18 ERROR JobScheduler: Error running job streaming job 1632352398000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:13:20 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:13:20 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:13:20 ERROR JobScheduler: Error running job streaming job 1632352400000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:13:22 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:13:22 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:13:22 ERROR JobScheduler: Error running job streaming job 1632352402000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:13:24 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:13:24 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:13:24 ERROR JobScheduler: Error running job streaming job 1632352404000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:13:26 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:13:26 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:13:26 ERROR JobScheduler: Error running job streaming job 1632352406000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:13:28 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:13:28 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:13:28 ERROR JobScheduler: Error running job streaming job 1632352408000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:13:30 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:13:30 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:13:30 ERROR JobScheduler: Error running job streaming job 1632352410000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:13:32 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:13:32 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:13:32 ERROR JobScheduler: Error running job streaming job 1632352412000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/09/22 23:13:33 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to localhost:9009 - java.net.ConnectException: Connection timed out (Connection timed out)\n",
      "\tat java.base/java.net.PlainSocketImpl.socketConnect(Native Method)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:399)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:242)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:224)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:609)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:558)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:454)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:231)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)\n",
      "\tat org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint.$anonfun$startReceiver$1(ReceiverTracker.scala:596)\n",
      "\tat org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint.$anonfun$startReceiver$1$adapted(ReceiverTracker.scala:586)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$submitJob$1(SparkContext.scala:2345)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:13:34 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:13:34 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:13:34 ERROR JobScheduler: Error running job streaming job 1632352414000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:13:36 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:13:36 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:13:36 ERROR JobScheduler: Error running job streaming job 1632352416000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:13:38 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:13:38 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:13:38 ERROR JobScheduler: Error running job streaming job 1632352418000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:13:40 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:13:40 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:13:40 ERROR JobScheduler: Error running job streaming job 1632352420000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:13:42 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:13:42 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:13:42 ERROR JobScheduler: Error running job streaming job 1632352422000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:13:44 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:13:44 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:13:44 ERROR JobScheduler: Error running job streaming job 1632352424000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:13:46 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:13:46 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:13:46 ERROR JobScheduler: Error running job streaming job 1632352426000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:13:48 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:13:48 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:13:48 ERROR JobScheduler: Error running job streaming job 1632352428000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:13:50 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:13:50 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:13:50 ERROR JobScheduler: Error running job streaming job 1632352430000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:13:52 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:13:52 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:13:52 ERROR JobScheduler: Error running job streaming job 1632352432000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:13:54 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:13:54 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:13:54 ERROR JobScheduler: Error running job streaming job 1632352434000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:13:56 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:13:56 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:13:56 ERROR JobScheduler: Error running job streaming job 1632352436000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:13:58 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:13:58 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:13:58 ERROR JobScheduler: Error running job streaming job 1632352438000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:14:00 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:14:00 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:14:00 ERROR JobScheduler: Error running job streaming job 1632352440000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:14:02 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:14:02 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:14:02 ERROR JobScheduler: Error running job streaming job 1632352442000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:14:04 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:14:04 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:14:04 ERROR JobScheduler: Error running job streaming job 1632352444000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:14:06 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:14:06 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:14:06 ERROR JobScheduler: Error running job streaming job 1632352446000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:14:08 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:14:08 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:14:08 ERROR JobScheduler: Error running job streaming job 1632352448000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:14:10 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:14:10 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:14:10 ERROR JobScheduler: Error running job streaming job 1632352450000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:14:12 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:14:12 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:14:12 ERROR JobScheduler: Error running job streaming job 1632352452000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:14:14 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:14:14 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:14:14 ERROR JobScheduler: Error running job streaming job 1632352454000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:14:16 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:14:16 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:14:16 ERROR JobScheduler: Error running job streaming job 1632352456000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:14:18 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:14:18 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:14:18 ERROR JobScheduler: Error running job streaming job 1632352458000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:14:20 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:14:20 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:14:20 ERROR JobScheduler: Error running job streaming job 1632352460000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:14:22 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:14:22 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:14:22 ERROR JobScheduler: Error running job streaming job 1632352462000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:14:24 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:14:24 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:14:24 ERROR JobScheduler: Error running job streaming job 1632352464000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:14:26 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:14:26 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:14:26 ERROR JobScheduler: Error running job streaming job 1632352466000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:14:28 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:14:28 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:14:28 ERROR JobScheduler: Error running job streaming job 1632352468000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:14:30 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:14:30 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:14:30 ERROR JobScheduler: Error running job streaming job 1632352470000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:14:32 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:14:32 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:14:32 ERROR JobScheduler: Error running job streaming job 1632352472000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:14:34 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:14:34 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:14:34 ERROR JobScheduler: Error running job streaming job 1632352474000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:14:36 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:14:36 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:14:36 ERROR JobScheduler: Error running job streaming job 1632352476000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:14:38 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:14:38 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:14:38 ERROR JobScheduler: Error running job streaming job 1632352478000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:14:40 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:14:40 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:14:40 ERROR JobScheduler: Error running job streaming job 1632352480000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:14:42 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:14:42 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:14:42 ERROR JobScheduler: Error running job streaming job 1632352482000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:14:44 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:14:44 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:14:44 ERROR JobScheduler: Error running job streaming job 1632352484000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:14:46 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:14:46 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:14:46 ERROR JobScheduler: Error running job streaming job 1632352486000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:14:48 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:14:48 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:14:48 ERROR JobScheduler: Error running job streaming job 1632352488000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:14:50 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:14:50 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:14:50 ERROR JobScheduler: Error running job streaming job 1632352490000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:14:52 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:14:52 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:14:52 ERROR JobScheduler: Error running job streaming job 1632352492000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:14:54 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:14:54 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:14:54 ERROR JobScheduler: Error running job streaming job 1632352494000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:14:56 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:14:56 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:14:56 ERROR JobScheduler: Error running job streaming job 1632352496000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:14:58 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:14:58 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:14:58 ERROR JobScheduler: Error running job streaming job 1632352498000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:15:00 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:15:00 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:15:00 ERROR JobScheduler: Error running job streaming job 1632352500000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:15:02 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:15:02 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:15:02 ERROR JobScheduler: Error running job streaming job 1632352502000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:15:04 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:15:04 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:15:04 ERROR JobScheduler: Error running job streaming job 1632352504000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:15:06 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:15:06 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:15:06 ERROR JobScheduler: Error running job streaming job 1632352506000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:15:08 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:15:08 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:15:08 ERROR JobScheduler: Error running job streaming job 1632352508000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:15:10 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:15:10 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:15:10 ERROR JobScheduler: Error running job streaming job 1632352510000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:15:12 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:15:12 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:15:12 ERROR JobScheduler: Error running job streaming job 1632352512000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:15:14 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:15:14 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:15:14 ERROR JobScheduler: Error running job streaming job 1632352514000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:15:16 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:15:16 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:15:16 ERROR JobScheduler: Error running job streaming job 1632352516000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:15:18 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:15:18 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:15:18 ERROR JobScheduler: Error running job streaming job 1632352518000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:15:20 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:15:20 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:15:20 ERROR JobScheduler: Error running job streaming job 1632352520000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:15:22 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:15:22 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:15:22 ERROR JobScheduler: Error running job streaming job 1632352522000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:15:24 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:15:24 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:15:24 ERROR JobScheduler: Error running job streaming job 1632352524000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:15:26 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:15:26 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:15:26 ERROR JobScheduler: Error running job streaming job 1632352526000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:15:28 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:15:28 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:15:28 ERROR JobScheduler: Error running job streaming job 1632352528000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:15:30 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:15:30 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:15:30 ERROR JobScheduler: Error running job streaming job 1632352530000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:15:32 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:15:32 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:15:32 ERROR JobScheduler: Error running job streaming job 1632352532000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:15:34 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:15:34 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:15:34 ERROR JobScheduler: Error running job streaming job 1632352534000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:15:36 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:15:36 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:15:36 ERROR JobScheduler: Error running job streaming job 1632352536000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:15:38 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:15:38 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:15:38 ERROR JobScheduler: Error running job streaming job 1632352538000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:15:40 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:15:40 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:15:40 ERROR JobScheduler: Error running job streaming job 1632352540000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:15:42 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:15:42 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:15:42 ERROR JobScheduler: Error running job streaming job 1632352542000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:15:44 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:15:44 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:15:44 ERROR JobScheduler: Error running job streaming job 1632352544000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:15:46 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:15:46 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:15:46 ERROR JobScheduler: Error running job streaming job 1632352546000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/09/22 23:15:46 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to localhost:9009 - java.net.ConnectException: Connection timed out (Connection timed out)\n",
      "\tat java.base/java.net.PlainSocketImpl.socketConnect(Native Method)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:399)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:242)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:224)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:609)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:558)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:454)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:231)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:15:48 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:15:48 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:15:48 ERROR JobScheduler: Error running job streaming job 1632352548000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/09/22 23:15:48 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to localhost:9009 - java.net.ConnectException: Connection refused (Connection refused)\n",
      "\tat java.base/java.net.PlainSocketImpl.socketConnect(Native Method)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:399)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:242)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:224)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:609)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:558)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:454)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:231)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:15:50 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:15:50 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:15:50 ERROR JobScheduler: Error running job streaming job 1632352550000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/09/22 23:15:50 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to localhost:9009 - java.net.ConnectException: Connection refused (Connection refused)\n",
      "\tat java.base/java.net.PlainSocketImpl.socketConnect(Native Method)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:399)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:242)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:224)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:609)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:558)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:454)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:231)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:15:52 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:15:52 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:15:52 ERROR JobScheduler: Error running job streaming job 1632352552000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/09/22 23:15:52 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to localhost:9009 - java.net.ConnectException: Connection refused (Connection refused)\n",
      "\tat java.base/java.net.PlainSocketImpl.socketConnect(Native Method)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:399)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:242)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:224)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:609)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:558)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:454)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:231)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:15:54 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:15:54 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:15:54 ERROR JobScheduler: Error running job streaming job 1632352554000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/09/22 23:15:54 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to localhost:9009 - java.net.ConnectException: Connection refused (Connection refused)\n",
      "\tat java.base/java.net.PlainSocketImpl.socketConnect(Native Method)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:399)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:242)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:224)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:609)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:558)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:454)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:231)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:15:56 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:15:56 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:15:56 ERROR JobScheduler: Error running job streaming job 1632352556000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/09/22 23:15:56 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to localhost:9009 - java.net.ConnectException: Connection refused (Connection refused)\n",
      "\tat java.base/java.net.PlainSocketImpl.socketConnect(Native Method)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:399)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:242)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:224)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:609)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:558)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:454)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:231)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:15:58 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:15:58 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:15:58 ERROR JobScheduler: Error running job streaming job 1632352558000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/09/22 23:15:58 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to localhost:9009 - java.net.ConnectException: Connection refused (Connection refused)\n",
      "\tat java.base/java.net.PlainSocketImpl.socketConnect(Native Method)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:399)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:242)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:224)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:609)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:558)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:454)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:231)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:16:00 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:16:00 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:16:00 ERROR JobScheduler: Error running job streaming job 1632352560000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/09/22 23:16:00 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to localhost:9009 - java.net.ConnectException: Connection refused (Connection refused)\n",
      "\tat java.base/java.net.PlainSocketImpl.socketConnect(Native Method)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:399)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:242)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:224)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:609)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:558)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:454)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:231)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:16:02 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:16:02 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:16:02 ERROR JobScheduler: Error running job streaming job 1632352562000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/09/22 23:16:02 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to localhost:9009 - java.net.ConnectException: Connection refused (Connection refused)\n",
      "\tat java.base/java.net.PlainSocketImpl.socketConnect(Native Method)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:399)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:242)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:224)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:609)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:558)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:454)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:231)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:16:04 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:16:04 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:16:04 ERROR JobScheduler: Error running job streaming job 1632352564000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/09/22 23:16:04 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to localhost:9009 - java.net.ConnectException: Connection refused (Connection refused)\n",
      "\tat java.base/java.net.PlainSocketImpl.socketConnect(Native Method)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:399)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:242)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:224)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:609)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:558)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:454)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:231)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:16:06 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:16:06 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:16:06 ERROR JobScheduler: Error running job streaming job 1632352566000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/09/22 23:16:07 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to localhost:9009 - java.net.ConnectException: Connection refused (Connection refused)\n",
      "\tat java.base/java.net.PlainSocketImpl.socketConnect(Native Method)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:399)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:242)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:224)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:609)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:558)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:454)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:231)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:16:08 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:16:08 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:16:08 ERROR JobScheduler: Error running job streaming job 1632352568000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/09/22 23:16:09 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to localhost:9009 - java.net.ConnectException: Connection refused (Connection refused)\n",
      "\tat java.base/java.net.PlainSocketImpl.socketConnect(Native Method)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:399)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:242)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:224)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:609)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:558)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:454)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:231)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:16:10 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:16:10 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:16:10 ERROR JobScheduler: Error running job streaming job 1632352570000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/09/22 23:16:11 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to localhost:9009 - java.net.ConnectException: Connection refused (Connection refused)\n",
      "\tat java.base/java.net.PlainSocketImpl.socketConnect(Native Method)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:399)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:242)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:224)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:609)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:558)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:454)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:231)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:16:12 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:16:12 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:16:12 ERROR JobScheduler: Error running job streaming job 1632352572000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/09/22 23:16:13 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to localhost:9009 - java.net.ConnectException: Connection refused (Connection refused)\n",
      "\tat java.base/java.net.PlainSocketImpl.socketConnect(Native Method)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:399)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:242)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:224)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:609)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:558)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:454)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:231)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:16:14 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:16:14 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:16:14 ERROR JobScheduler: Error running job streaming job 1632352574000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/09/22 23:16:15 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to localhost:9009 - java.net.ConnectException: Connection refused (Connection refused)\n",
      "\tat java.base/java.net.PlainSocketImpl.socketConnect(Native Method)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:399)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:242)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:224)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:609)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:558)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:454)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:231)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:16:16 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:16:16 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:16:16 ERROR JobScheduler: Error running job streaming job 1632352576000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/09/22 23:16:17 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to localhost:9009 - java.net.ConnectException: Connection refused (Connection refused)\n",
      "\tat java.base/java.net.PlainSocketImpl.socketConnect(Native Method)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:399)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:242)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:224)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:609)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:558)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:454)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:231)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:16:18 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:16:18 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:16:18 ERROR JobScheduler: Error running job streaming job 1632352578000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/09/22 23:16:19 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to localhost:9009 - java.net.ConnectException: Connection refused (Connection refused)\n",
      "\tat java.base/java.net.PlainSocketImpl.socketConnect(Native Method)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:399)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:242)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:224)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:609)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:558)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:454)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:231)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:16:20 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:16:20 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:16:20 ERROR JobScheduler: Error running job streaming job 1632352580000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/09/22 23:16:21 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to localhost:9009 - java.net.ConnectException: Connection refused (Connection refused)\n",
      "\tat java.base/java.net.PlainSocketImpl.socketConnect(Native Method)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:399)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:242)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:224)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:609)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:558)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:454)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:231)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:16:22 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:16:22 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:16:22 ERROR JobScheduler: Error running job streaming job 1632352582000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/09/22 23:16:23 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to localhost:9009 - java.net.ConnectException: Connection refused (Connection refused)\n",
      "\tat java.base/java.net.PlainSocketImpl.socketConnect(Native Method)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:399)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:242)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:224)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:609)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:558)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:454)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:231)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:16:24 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:16:24 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:16:24 ERROR JobScheduler: Error running job streaming job 1632352584000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/09/22 23:16:25 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to localhost:9009 - java.net.ConnectException: Connection refused (Connection refused)\n",
      "\tat java.base/java.net.PlainSocketImpl.socketConnect(Native Method)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:399)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:242)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:224)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:609)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:558)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:454)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:231)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:16:26 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:16:26 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:16:26 ERROR JobScheduler: Error running job streaming job 1632352586000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/09/22 23:16:27 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to localhost:9009 - java.net.ConnectException: Connection refused (Connection refused)\n",
      "\tat java.base/java.net.PlainSocketImpl.socketConnect(Native Method)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:399)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:242)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:224)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:609)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:558)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:454)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:231)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:16:28 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:16:28 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:16:28 ERROR JobScheduler: Error running job streaming job 1632352588000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/09/22 23:16:29 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to localhost:9009 - java.net.ConnectException: Connection refused (Connection refused)\n",
      "\tat java.base/java.net.PlainSocketImpl.socketConnect(Native Method)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:399)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:242)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:224)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:609)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:558)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:454)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:231)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:16:30 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:16:30 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:16:30 ERROR JobScheduler: Error running job streaming job 1632352590000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/09/22 23:16:31 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to localhost:9009 - java.net.ConnectException: Connection refused (Connection refused)\n",
      "\tat java.base/java.net.PlainSocketImpl.socketConnect(Native Method)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:399)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:242)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:224)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:609)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:558)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:454)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:231)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:16:32 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:16:32 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:16:32 ERROR JobScheduler: Error running job streaming job 1632352592000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/09/22 23:16:33 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to localhost:9009 - java.net.ConnectException: Connection refused (Connection refused)\n",
      "\tat java.base/java.net.PlainSocketImpl.socketConnect(Native Method)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:399)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:242)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:224)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:609)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:558)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:454)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:231)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:16:34 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:16:34 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:16:34 ERROR JobScheduler: Error running job streaming job 1632352594000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/09/22 23:16:35 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to localhost:9009 - java.net.ConnectException: Connection refused (Connection refused)\n",
      "\tat java.base/java.net.PlainSocketImpl.socketConnect(Native Method)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:399)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:242)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:224)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:609)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:558)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:454)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:231)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:16:36 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:16:36 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:16:36 ERROR JobScheduler: Error running job streaming job 1632352596000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/09/22 23:16:37 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to localhost:9009 - java.net.ConnectException: Connection refused (Connection refused)\n",
      "\tat java.base/java.net.PlainSocketImpl.socketConnect(Native Method)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:399)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:242)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:224)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:609)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:558)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:454)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:231)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:16:38 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:16:38 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:16:38 ERROR JobScheduler: Error running job streaming job 1632352598000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/09/22 23:16:39 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to localhost:9009 - java.net.ConnectException: Connection refused (Connection refused)\n",
      "\tat java.base/java.net.PlainSocketImpl.socketConnect(Native Method)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:399)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:242)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:224)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:609)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:558)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:454)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:231)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:16:40 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:16:40 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:16:40 ERROR JobScheduler: Error running job streaming job 1632352600000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/09/22 23:16:41 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to localhost:9009 - java.net.ConnectException: Connection refused (Connection refused)\n",
      "\tat java.base/java.net.PlainSocketImpl.socketConnect(Native Method)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:399)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:242)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:224)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:609)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:558)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:454)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:231)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:16:42 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:16:42 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:16:42 ERROR JobScheduler: Error running job streaming job 1632352602000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/09/22 23:16:43 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to localhost:9009 - java.net.ConnectException: Connection refused (Connection refused)\n",
      "\tat java.base/java.net.PlainSocketImpl.socketConnect(Native Method)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:399)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:242)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:224)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:609)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:558)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:454)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:231)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:16:44 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:16:44 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:16:44 ERROR JobScheduler: Error running job streaming job 1632352604000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/09/22 23:16:45 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to localhost:9009 - java.net.ConnectException: Connection refused (Connection refused)\n",
      "\tat java.base/java.net.PlainSocketImpl.socketConnect(Native Method)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:399)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:242)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:224)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:609)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:558)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:454)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:231)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:16:46 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:16:46 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:16:46 ERROR JobScheduler: Error running job streaming job 1632352606000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/09/22 23:16:47 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to localhost:9009 - java.net.ConnectException: Connection refused (Connection refused)\n",
      "\tat java.base/java.net.PlainSocketImpl.socketConnect(Native Method)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:399)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:242)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:224)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:609)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:558)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:454)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:231)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:16:48 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:16:48 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:16:48 ERROR JobScheduler: Error running job streaming job 1632352608000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/09/22 23:16:49 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to localhost:9009 - java.net.ConnectException: Connection refused (Connection refused)\n",
      "\tat java.base/java.net.PlainSocketImpl.socketConnect(Native Method)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:399)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:242)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:224)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:609)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:558)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:454)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:231)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:16:50 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:16:50 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:16:50 ERROR JobScheduler: Error running job streaming job 1632352610000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:16:52 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:16:52 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:16:52 ERROR JobScheduler: Error running job streaming job 1632352612000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:16:54 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:16:54 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:16:54 ERROR JobScheduler: Error running job streaming job 1632352614000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:16:56 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:16:56 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:16:56 ERROR JobScheduler: Error running job streaming job 1632352616000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:16:58 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:16:58 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:16:58 ERROR JobScheduler: Error running job streaming job 1632352618000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:17:00 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:17:00 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:17:00 ERROR JobScheduler: Error running job streaming job 1632352620000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:17:02 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:17:02 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:17:02 ERROR JobScheduler: Error running job streaming job 1632352622000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:17:04 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:17:04 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:17:04 ERROR JobScheduler: Error running job streaming job 1632352624000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:17:06 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:17:06 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:17:06 ERROR JobScheduler: Error running job streaming job 1632352626000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:17:08 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:17:08 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:17:08 ERROR JobScheduler: Error running job streaming job 1632352628000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:17:10 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:17:10 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:17:10 ERROR JobScheduler: Error running job streaming job 1632352630000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:17:12 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:17:12 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:17:12 ERROR JobScheduler: Error running job streaming job 1632352632000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:17:14 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:17:14 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:17:14 ERROR JobScheduler: Error running job streaming job 1632352634000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:17:16 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:17:16 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:17:16 ERROR JobScheduler: Error running job streaming job 1632352636000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:17:18 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:17:18 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:17:18 ERROR JobScheduler: Error running job streaming job 1632352638000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:17:20 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:17:20 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:17:20 ERROR JobScheduler: Error running job streaming job 1632352640000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:17:22 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:17:22 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:17:22 ERROR JobScheduler: Error running job streaming job 1632352642000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:17:24 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:17:24 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:17:24 ERROR JobScheduler: Error running job streaming job 1632352644000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:17:26 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:17:26 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:17:26 ERROR JobScheduler: Error running job streaming job 1632352646000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:17:28 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:17:28 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:17:28 ERROR JobScheduler: Error running job streaming job 1632352648000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:17:30 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:17:30 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:17:30 ERROR JobScheduler: Error running job streaming job 1632352650000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:17:32 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:17:32 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:17:32 ERROR JobScheduler: Error running job streaming job 1632352652000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:17:34 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:17:34 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:17:34 ERROR JobScheduler: Error running job streaming job 1632352654000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:17:36 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:17:36 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:17:36 ERROR JobScheduler: Error running job streaming job 1632352656000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:17:38 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:17:38 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:17:38 ERROR JobScheduler: Error running job streaming job 1632352658000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:17:40 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:17:40 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:17:40 ERROR JobScheduler: Error running job streaming job 1632352660000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:17:42 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:17:42 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:17:42 ERROR JobScheduler: Error running job streaming job 1632352662000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:17:44 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:17:44 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:17:44 ERROR JobScheduler: Error running job streaming job 1632352664000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:17:46 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:17:46 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:17:46 ERROR JobScheduler: Error running job streaming job 1632352666000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:17:48 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:17:48 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:17:48 ERROR JobScheduler: Error running job streaming job 1632352668000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:17:50 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:17:50 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:17:50 ERROR JobScheduler: Error running job streaming job 1632352670000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:17:52 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:17:52 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:17:52 ERROR JobScheduler: Error running job streaming job 1632352672000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:17:54 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:17:54 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:17:54 ERROR JobScheduler: Error running job streaming job 1632352674000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:17:56 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:17:56 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:17:56 ERROR JobScheduler: Error running job streaming job 1632352676000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:17:58 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:17:58 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:17:58 ERROR JobScheduler: Error running job streaming job 1632352678000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:18:00 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:18:00 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:18:00 ERROR JobScheduler: Error running job streaming job 1632352680000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:18:02 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:18:02 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:18:02 ERROR JobScheduler: Error running job streaming job 1632352682000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:18:04 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:18:04 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:18:04 ERROR JobScheduler: Error running job streaming job 1632352684000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:18:06 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:18:06 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:18:06 ERROR JobScheduler: Error running job streaming job 1632352686000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:18:08 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:18:08 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:18:08 ERROR JobScheduler: Error running job streaming job 1632352688000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:18:10 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:18:10 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:18:10 ERROR JobScheduler: Error running job streaming job 1632352690000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:18:12 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:18:12 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:18:12 ERROR JobScheduler: Error running job streaming job 1632352692000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:18:14 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:18:14 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:18:14 ERROR JobScheduler: Error running job streaming job 1632352694000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:18:16 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:18:16 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:18:16 ERROR JobScheduler: Error running job streaming job 1632352696000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:18:18 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:18:18 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:18:18 ERROR JobScheduler: Error running job streaming job 1632352698000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:18:20 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:18:20 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:18:20 ERROR JobScheduler: Error running job streaming job 1632352700000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:18:22 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:18:22 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:18:22 ERROR JobScheduler: Error running job streaming job 1632352702000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:18:24 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:18:24 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:18:24 ERROR JobScheduler: Error running job streaming job 1632352704000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:18:26 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:18:26 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:18:26 ERROR JobScheduler: Error running job streaming job 1632352706000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:18:28 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:18:28 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:18:28 ERROR JobScheduler: Error running job streaming job 1632352708000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-09-22 23:18:30 -----------\n",
      "Get spark sql singleton context from the current context ----------- 2021-09-22 23:18:30 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 23:18:30 ERROR JobScheduler: Error running job streaming job 1632352710000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/tmp/ipykernel_12505/1292132479.py\", line 12, in process_rdd\n",
      "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/context.py\", line 369, in createDataFrame\n",
      "    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n",
      "    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1589, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    }
   ],
   "source": [
    "# divide cada Tweet en palabras\n",
    "words = dataStream.flatMap(lambda line: line.split(\" \"))\n",
    "\n",
    "# filtra las palabras para obtener solo hashtags, luego mapea cada hashtag para que sea un par de (hashtag,1)\n",
    "hashtags = words.filter(lambda w: '#' in w).map(lambda x: (x, 1))\n",
    "\n",
    "# agrega la cuenta de cada hashtag a su última cuenta\n",
    "tags_totals = hashtags.updateStateByKey(aggregate_tags_count)\n",
    "\n",
    "# procesa cada RDD generado en cada intervalo\n",
    "tags_totals.foreachRDD(process_rdd)\n",
    "\n",
    "# comienza la computación de streaming\n",
    "ssc.start()\n",
    "\n",
    "# espera que la transmisión termine\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7c10c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
