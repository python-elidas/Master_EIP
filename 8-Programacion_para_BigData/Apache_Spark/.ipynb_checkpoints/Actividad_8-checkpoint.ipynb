{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14eabd21",
   "metadata": {},
   "source": [
    "# Actividad 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c43c10",
   "metadata": {},
   "source": [
    "### Analisis y entendimiento del Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cde7b4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# empecemos importando las librerias necesarisas\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, round\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator as MCE\n",
    "from pyspark.ml.classification import DecisionTreeClassifier as DTC\n",
    "from pyspark.ml.classification import GBTClassifier as GBTC\n",
    "from pyspark.ml.classification import RandomForestClassifier as RFC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ffda4e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.1.2-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "21/09/23 20:52:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# Ceemos la sesion:\n",
    "spark = SparkSession.builder.appName('IrisDS').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfb56b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+------------+-------------+------------+-----------+\n",
      "| Id|SepalLengthCm|SepalWidthCm|PetalLengthCm|PetalWidthCm|    Species|\n",
      "+---+-------------+------------+-------------+------------+-----------+\n",
      "|  1|          5.1|         3.5|          1.4|         0.2|Iris-setosa|\n",
      "|  2|          4.9|         3.0|          1.4|         0.2|Iris-setosa|\n",
      "|  3|          4.7|         3.2|          1.3|         0.2|Iris-setosa|\n",
      "|  4|          4.6|         3.1|          1.5|         0.2|Iris-setosa|\n",
      "|  5|          5.0|         3.6|          1.4|         0.2|Iris-setosa|\n",
      "|  6|          5.4|         3.9|          1.7|         0.4|Iris-setosa|\n",
      "|  7|          4.6|         3.4|          1.4|         0.3|Iris-setosa|\n",
      "|  8|          5.0|         3.4|          1.5|         0.2|Iris-setosa|\n",
      "|  9|          4.4|         2.9|          1.4|         0.2|Iris-setosa|\n",
      "| 10|          4.9|         3.1|          1.5|         0.1|Iris-setosa|\n",
      "+---+-------------+------------+-------------+------------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#leamos el data set\n",
    "df = spark.read.csv('Iris_DataSheet.csv', header = True).cache()\n",
    "# primer vistazo\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a4bd20b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lineas del data set:\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59ee67cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Id', 'string'),\n",
       " ('SepalLengthCm', 'string'),\n",
       " ('SepalWidthCm', 'string'),\n",
       " ('PetalLengthCm', 'string'),\n",
       " ('PetalWidthCm', 'string'),\n",
       " ('Species', 'string')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# columnas y su tipo:\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7feb4adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+------------+-------------+------------+-----------+\n",
      "| Id|SepalLengthCm|SepalWidthCm|PetalLengthCm|PetalWidthCm|    Species|\n",
      "+---+-------------+------------+-------------+------------+-----------+\n",
      "|  1|          5.1|         3.5|          1.4|         0.2|Iris-setosa|\n",
      "|  2|          4.9|         3.0|          1.4|         0.2|Iris-setosa|\n",
      "|  3|          4.7|         3.2|          1.3|         0.2|Iris-setosa|\n",
      "|  4|          4.6|         3.1|          1.5|         0.2|Iris-setosa|\n",
      "|  5|          5.0|         3.6|          1.4|         0.2|Iris-setosa|\n",
      "+---+-------------+------------+-------------+------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Adecuemos el data set dando el fromato adecuado a las columnas numéricas\n",
    "df = df.select(\n",
    "    col('Id').cast('int'),\n",
    "    col('SepalLengthCm').cast('float'),\n",
    "    col('SepalWidthCm').cast('float'),\n",
    "    col('PetalLengthCm').cast('float'),\n",
    "    col('PetalWidthCm').cast('float'),\n",
    "    col('Species')\n",
    ")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c41433f",
   "metadata": {},
   "source": [
    "## Transformar en vector los datos numéricos empleando Vector Assembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6fc3c3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lo primero, definamos que datos(columnas) se han de combertir en vector:\n",
    "req_feat = ['SepalLengthCm','SepalWidthCm','PetalLengthCm','PetalWidthCm']\n",
    "\n",
    "# creemos los vectores\n",
    "assembler = VectorAssembler(inputCols=req_feat, outputCol='Sizes')\n",
    "\n",
    "# añadamoslo al dataset\n",
    "df = assembler.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b659a3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+------------+-------------+------------+-----------+--------------------+\n",
      "| Id|SepalLengthCm|SepalWidthCm|PetalLengthCm|PetalWidthCm|    Species|               Sizes|\n",
      "+---+-------------+------------+-------------+------------+-----------+--------------------+\n",
      "|  1|          5.1|         3.5|          1.4|         0.2|Iris-setosa|[5.09999990463256...|\n",
      "|  2|          4.9|         3.0|          1.4|         0.2|Iris-setosa|[4.90000009536743...|\n",
      "|  3|          4.7|         3.2|          1.3|         0.2|Iris-setosa|[4.69999980926513...|\n",
      "|  4|          4.6|         3.1|          1.5|         0.2|Iris-setosa|[4.59999990463256...|\n",
      "|  5|          5.0|         3.6|          1.4|         0.2|Iris-setosa|[5.0,3.5999999046...|\n",
      "|  6|          5.4|         3.9|          1.7|         0.4|Iris-setosa|[5.40000009536743...|\n",
      "|  7|          4.6|         3.4|          1.4|         0.3|Iris-setosa|[4.59999990463256...|\n",
      "|  8|          5.0|         3.4|          1.5|         0.2|Iris-setosa|[5.0,3.4000000953...|\n",
      "|  9|          4.4|         2.9|          1.4|         0.2|Iris-setosa|[4.40000009536743...|\n",
      "| 10|          4.9|         3.1|          1.5|         0.1|Iris-setosa|[4.90000009536743...|\n",
      "+---+-------------+------------+-------------+------------+-----------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58bec06d",
   "metadata": {},
   "source": [
    "## Transformar los datos correspondientes a la columna especies usando StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b16462fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convirtamos la columna 'Species' en una columnas numérica\n",
    "df = StringIndexer(inputCol='Species', outputCol='SpeciesID').fit(df).transform(df)\n",
    "# A setosa le asignará el 1, a versicolor el 2 y a virginica, el 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07d9008e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+------------+-------------+------------+-----------+--------------------+---------+\n",
      "| Id|SepalLengthCm|SepalWidthCm|PetalLengthCm|PetalWidthCm|    Species|               Sizes|SpeciesID|\n",
      "+---+-------------+------------+-------------+------------+-----------+--------------------+---------+\n",
      "|  1|          5.1|         3.5|          1.4|         0.2|Iris-setosa|[5.09999990463256...|      0.0|\n",
      "|  2|          4.9|         3.0|          1.4|         0.2|Iris-setosa|[4.90000009536743...|      0.0|\n",
      "|  3|          4.7|         3.2|          1.3|         0.2|Iris-setosa|[4.69999980926513...|      0.0|\n",
      "|  4|          4.6|         3.1|          1.5|         0.2|Iris-setosa|[4.59999990463256...|      0.0|\n",
      "|  5|          5.0|         3.6|          1.4|         0.2|Iris-setosa|[5.0,3.5999999046...|      0.0|\n",
      "|  6|          5.4|         3.9|          1.7|         0.4|Iris-setosa|[5.40000009536743...|      0.0|\n",
      "|  7|          4.6|         3.4|          1.4|         0.3|Iris-setosa|[4.59999990463256...|      0.0|\n",
      "|  8|          5.0|         3.4|          1.5|         0.2|Iris-setosa|[5.0,3.4000000953...|      0.0|\n",
      "|  9|          4.4|         2.9|          1.4|         0.2|Iris-setosa|[4.40000009536743...|      0.0|\n",
      "| 10|          4.9|         3.1|          1.5|         0.1|Iris-setosa|[4.90000009536743...|      0.0|\n",
      "+---+-------------+------------+-------------+------------+-----------+--------------------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0167f75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Id: int, SepalLengthCm: float, SepalWidthCm: float, PetalLengthCm: float, PetalWidthCm: float, Sizes: vector, SpeciesID: double]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#quitemos la columna Species\n",
    "df.drop('Species')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14355a32",
   "metadata": {},
   "source": [
    "**Creemos ahora las particiones Train Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac62cfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#empelaremos el 85% del data set para Train y el 15% para test\n",
    "(train, test) = df.randomSplit([0.85, 0.15])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62269037",
   "metadata": {},
   "source": [
    "## Calcular la predicción y la precisión del modelo empleando Decision Tree Classifier (DTC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d29eef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Antes de nada, creemos el modelo de prediccion\n",
    "dtc = DTC(labelCol='SpeciesID', featuresCol='Sizes',maxDepth=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef2f5e7",
   "metadata": {},
   "source": [
    "Aumentando el valor de 'maxDepth' de 5 a 10 la prediccion del modelo mejora, al menos en este caso, de ahi para arriba parece que se crea algo de overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "88d891c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenemos el modelo:\n",
    "dtc_model = dtc.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fc8f2474",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>SepalLengthCm</th>\n",
       "      <th>SepalWidthCm</th>\n",
       "      <th>PetalLengthCm</th>\n",
       "      <th>PetalWidthCm</th>\n",
       "      <th>Species</th>\n",
       "      <th>Sizes</th>\n",
       "      <th>SpeciesID</th>\n",
       "      <th>rawPrediction</th>\n",
       "      <th>probability</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>5.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.4</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>[5.400000095367432, 3.9000000953674316, 1.7000...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[44.0, 0.0, 0.0]</td>\n",
       "      <td>[1.0, 0.0, 0.0]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12</td>\n",
       "      <td>4.8</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>[4.800000190734863, 3.4000000953674316, 1.6000...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[44.0, 0.0, 0.0]</td>\n",
       "      <td>[1.0, 0.0, 0.0]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>4.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>[4.900000095367432, 3.0999999046325684, 1.5, 0...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[44.0, 0.0, 0.0]</td>\n",
       "      <td>[1.0, 0.0, 0.0]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>43</td>\n",
       "      <td>4.4</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>[4.400000095367432, 3.200000047683716, 1.29999...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[44.0, 0.0, 0.0]</td>\n",
       "      <td>[1.0, 0.0, 0.0]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>45</td>\n",
       "      <td>5.1</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.4</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>[5.099999904632568, 3.799999952316284, 1.89999...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[44.0, 0.0, 0.0]</td>\n",
       "      <td>[1.0, 0.0, 0.0]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm      Species  \\\n",
       "0   6            5.4           3.9            1.7           0.4  Iris-setosa   \n",
       "1  12            4.8           3.4            1.6           0.2  Iris-setosa   \n",
       "2  38            4.9           3.1            1.5           0.1  Iris-setosa   \n",
       "3  43            4.4           3.2            1.3           0.2  Iris-setosa   \n",
       "4  45            5.1           3.8            1.9           0.4  Iris-setosa   \n",
       "\n",
       "                                               Sizes  SpeciesID  \\\n",
       "0  [5.400000095367432, 3.9000000953674316, 1.7000...        0.0   \n",
       "1  [4.800000190734863, 3.4000000953674316, 1.6000...        0.0   \n",
       "2  [4.900000095367432, 3.0999999046325684, 1.5, 0...        0.0   \n",
       "3  [4.400000095367432, 3.200000047683716, 1.29999...        0.0   \n",
       "4  [5.099999904632568, 3.799999952316284, 1.89999...        0.0   \n",
       "\n",
       "      rawPrediction      probability  prediction  \n",
       "0  [44.0, 0.0, 0.0]  [1.0, 0.0, 0.0]         0.0  \n",
       "1  [44.0, 0.0, 0.0]  [1.0, 0.0, 0.0]         0.0  \n",
       "2  [44.0, 0.0, 0.0]  [1.0, 0.0, 0.0]         0.0  \n",
       "3  [44.0, 0.0, 0.0]  [1.0, 0.0, 0.0]         0.0  \n",
       "4  [44.0, 0.0, 0.0]  [1.0, 0.0, 0.0]         0.0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generemos una prediccion:\n",
    "dtc_pred = dtc_model.transform(test)\n",
    "dtc_pred.toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3199f114",
   "metadata": {},
   "source": [
    "### Evaluemos el modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f6c56fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuremos el evaluador:\n",
    "evaluator = MCE(labelCol='SpeciesID', predictionCol='prediction', metricName='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cd2542ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DTC test accuracy: 0.9444444444444444\n"
     ]
    }
   ],
   "source": [
    "#  Ahora si, evaluemos el modelo:\n",
    "accuracy = evaluator.evaluate(dtc_pred)\n",
    "print(f'DTC test accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c430413",
   "metadata": {},
   "source": [
    "## Calcular la predicción y la precisión del modelo empleando Gradient-boosted tree classifier GBTC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "851ee6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creemos el modelo de prediccion:\n",
    "gbtc = GBTC(labelCol='SpeciesID', featuresCol='Sizes', maxIter=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e833403",
   "metadata": {},
   "source": [
    "**Como el modelo GBTC solo permite trabajar con predicciones binarias, será necesario adecuar los datos:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "748de7ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>SepalLengthCm</th>\n",
       "      <th>SepalWidthCm</th>\n",
       "      <th>PetalLengthCm</th>\n",
       "      <th>PetalWidthCm</th>\n",
       "      <th>Species</th>\n",
       "      <th>Sizes</th>\n",
       "      <th>SpeciesID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>[5.099999904632568, 3.5, 1.399999976158142, 0....</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>[4.900000095367432, 3.0, 1.399999976158142, 0....</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>[4.699999809265137, 3.200000047683716, 1.29999...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>[4.599999904632568, 3.0999999046325684, 1.5, 0...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>[5.0, 3.5999999046325684, 1.399999976158142, 0...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm      Species  \\\n",
       "0   1            5.1           3.5            1.4           0.2  Iris-setosa   \n",
       "1   2            4.9           3.0            1.4           0.2  Iris-setosa   \n",
       "2   3            4.7           3.2            1.3           0.2  Iris-setosa   \n",
       "3   4            4.6           3.1            1.5           0.2  Iris-setosa   \n",
       "4   5            5.0           3.6            1.4           0.2  Iris-setosa   \n",
       "\n",
       "                                               Sizes  SpeciesID  \n",
       "0  [5.099999904632568, 3.5, 1.399999976158142, 0....        0.0  \n",
       "1  [4.900000095367432, 3.0, 1.399999976158142, 0....        0.0  \n",
       "2  [4.699999809265137, 3.200000047683716, 1.29999...        0.0  \n",
       "3  [4.599999904632568, 3.0999999046325684, 1.5, 0...        0.0  \n",
       "4  [5.0, 3.5999999046325684, 1.399999976158142, 0...        0.0  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convirtamos el data frame que tenemos a pandas:\n",
    "pdf = df.toPandas()\n",
    "pdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514c9e37",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0838c12f",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8b9a9dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/23 20:53:02 ERROR Executor: Exception in task 0.0 in stage 27.0 (TID 27)\n",
      "java.lang.IllegalArgumentException: requirement failed: GBTClassifier was given dataset with invalid label 2.0.  Labels must be in {0,1}; note that GBTClassifier currently only supports binary classification.\n",
      "\tat scala.Predef$.require(Predef.scala:281)\n",
      "\tat org.apache.spark.ml.classification.GBTClassifier.$anonfun$train$2(GBTClassifier.scala:177)\n",
      "\tat org.apache.spark.ml.classification.GBTClassifier.$anonfun$train$2$adapted(GBTClassifier.scala:174)\n",
      "\tat org.apache.spark.ml.PredictorParams.$anonfun$extractInstances$2(Predictor.scala:96)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n",
      "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:162)\n",
      "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:160)\n",
      "\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1429)\n",
      "\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:219)\n",
      "\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:219)\n",
      "\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1429)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$2(RDD.scala:1207)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2290)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/09/23 20:53:02 WARN TaskSetManager: Lost task 0.0 in stage 27.0 (TID 27) (1b3dec28ceb1 executor driver): java.lang.IllegalArgumentException: requirement failed: GBTClassifier was given dataset with invalid label 2.0.  Labels must be in {0,1}; note that GBTClassifier currently only supports binary classification.\n",
      "\tat scala.Predef$.require(Predef.scala:281)\n",
      "\tat org.apache.spark.ml.classification.GBTClassifier.$anonfun$train$2(GBTClassifier.scala:177)\n",
      "\tat org.apache.spark.ml.classification.GBTClassifier.$anonfun$train$2$adapted(GBTClassifier.scala:174)\n",
      "\tat org.apache.spark.ml.PredictorParams.$anonfun$extractInstances$2(Predictor.scala:96)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n",
      "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:162)\n",
      "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:160)\n",
      "\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1429)\n",
      "\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:219)\n",
      "\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:219)\n",
      "\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1429)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$2(RDD.scala:1207)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2290)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "21/09/23 20:53:02 ERROR TaskSetManager: Task 0 in stage 27.0 failed 1 times; aborting job\n",
      "21/09/23 20:53:02 ERROR Instrumentation: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 27.0 failed 1 times, most recent failure: Lost task 0.0 in stage 27.0 (TID 27) (1b3dec28ceb1 executor driver): java.lang.IllegalArgumentException: requirement failed: GBTClassifier was given dataset with invalid label 2.0.  Labels must be in {0,1}; note that GBTClassifier currently only supports binary classification.\n",
      "\tat scala.Predef$.require(Predef.scala:281)\n",
      "\tat org.apache.spark.ml.classification.GBTClassifier.$anonfun$train$2(GBTClassifier.scala:177)\n",
      "\tat org.apache.spark.ml.classification.GBTClassifier.$anonfun$train$2$adapted(GBTClassifier.scala:174)\n",
      "\tat org.apache.spark.ml.PredictorParams.$anonfun$extractInstances$2(Predictor.scala:96)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n",
      "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:162)\n",
      "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:160)\n",
      "\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1429)\n",
      "\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:219)\n",
      "\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:219)\n",
      "\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1429)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$2(RDD.scala:1207)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2290)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2291)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$1(RDD.scala:1209)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n",
      "\tat org.apache.spark.rdd.RDD.aggregate(RDD.scala:1202)\n",
      "\tat org.apache.spark.ml.tree.impl.DecisionTreeMetadata$.buildMetadata(DecisionTreeMetadata.scala:125)\n",
      "\tat org.apache.spark.ml.tree.impl.GradientBoostedTrees$.boost(GradientBoostedTrees.scala:333)\n",
      "\tat org.apache.spark.ml.tree.impl.GradientBoostedTrees$.run(GradientBoostedTrees.scala:61)\n",
      "\tat org.apache.spark.ml.classification.GBTClassifier.$anonfun$train$1(GBTClassifier.scala:210)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
      "\tat org.apache.spark.ml.classification.GBTClassifier.train(GBTClassifier.scala:171)\n",
      "\tat org.apache.spark.ml.classification.GBTClassifier.train(GBTClassifier.scala:59)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:151)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:115)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.lang.IllegalArgumentException: requirement failed: GBTClassifier was given dataset with invalid label 2.0.  Labels must be in {0,1}; note that GBTClassifier currently only supports binary classification.\n",
      "\tat scala.Predef$.require(Predef.scala:281)\n",
      "\tat org.apache.spark.ml.classification.GBTClassifier.$anonfun$train$2(GBTClassifier.scala:177)\n",
      "\tat org.apache.spark.ml.classification.GBTClassifier.$anonfun$train$2$adapted(GBTClassifier.scala:174)\n",
      "\tat org.apache.spark.ml.PredictorParams.$anonfun$extractInstances$2(Predictor.scala:96)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n",
      "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:162)\n",
      "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:160)\n",
      "\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1429)\n",
      "\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:219)\n",
      "\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:219)\n",
      "\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1429)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$2(RDD.scala:1207)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2290)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\t... 1 more\n",
      "\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o259.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 27.0 failed 1 times, most recent failure: Lost task 0.0 in stage 27.0 (TID 27) (1b3dec28ceb1 executor driver): java.lang.IllegalArgumentException: requirement failed: GBTClassifier was given dataset with invalid label 2.0.  Labels must be in {0,1}; note that GBTClassifier currently only supports binary classification.\n\tat scala.Predef$.require(Predef.scala:281)\n\tat org.apache.spark.ml.classification.GBTClassifier.$anonfun$train$2(GBTClassifier.scala:177)\n\tat org.apache.spark.ml.classification.GBTClassifier.$anonfun$train$2$adapted(GBTClassifier.scala:174)\n\tat org.apache.spark.ml.PredictorParams.$anonfun$extractInstances$2(Predictor.scala:96)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:162)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:160)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1429)\n\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:219)\n\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:219)\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1429)\n\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$2(RDD.scala:1207)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2290)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2291)\n\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$1(RDD.scala:1209)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.aggregate(RDD.scala:1202)\n\tat org.apache.spark.ml.tree.impl.DecisionTreeMetadata$.buildMetadata(DecisionTreeMetadata.scala:125)\n\tat org.apache.spark.ml.tree.impl.GradientBoostedTrees$.boost(GradientBoostedTrees.scala:333)\n\tat org.apache.spark.ml.tree.impl.GradientBoostedTrees$.run(GradientBoostedTrees.scala:61)\n\tat org.apache.spark.ml.classification.GBTClassifier.$anonfun$train$1(GBTClassifier.scala:210)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n\tat org.apache.spark.ml.classification.GBTClassifier.train(GBTClassifier.scala:171)\n\tat org.apache.spark.ml.classification.GBTClassifier.train(GBTClassifier.scala:59)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:151)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:115)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.lang.IllegalArgumentException: requirement failed: GBTClassifier was given dataset with invalid label 2.0.  Labels must be in {0,1}; note that GBTClassifier currently only supports binary classification.\n\tat scala.Predef$.require(Predef.scala:281)\n\tat org.apache.spark.ml.classification.GBTClassifier.$anonfun$train$2(GBTClassifier.scala:177)\n\tat org.apache.spark.ml.classification.GBTClassifier.$anonfun$train$2$adapted(GBTClassifier.scala:174)\n\tat org.apache.spark.ml.PredictorParams.$anonfun$extractInstances$2(Predictor.scala:96)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:162)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:160)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1429)\n\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:219)\n\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:219)\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1429)\n\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$2(RDD.scala:1207)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2290)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_28250/3883920438.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Entrenemos el modelo:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mgbtc_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgbtc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \"\"\"\n\u001b[1;32m    331\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o259.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 27.0 failed 1 times, most recent failure: Lost task 0.0 in stage 27.0 (TID 27) (1b3dec28ceb1 executor driver): java.lang.IllegalArgumentException: requirement failed: GBTClassifier was given dataset with invalid label 2.0.  Labels must be in {0,1}; note that GBTClassifier currently only supports binary classification.\n\tat scala.Predef$.require(Predef.scala:281)\n\tat org.apache.spark.ml.classification.GBTClassifier.$anonfun$train$2(GBTClassifier.scala:177)\n\tat org.apache.spark.ml.classification.GBTClassifier.$anonfun$train$2$adapted(GBTClassifier.scala:174)\n\tat org.apache.spark.ml.PredictorParams.$anonfun$extractInstances$2(Predictor.scala:96)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:162)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:160)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1429)\n\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:219)\n\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:219)\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1429)\n\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$2(RDD.scala:1207)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2290)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2291)\n\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$1(RDD.scala:1209)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.aggregate(RDD.scala:1202)\n\tat org.apache.spark.ml.tree.impl.DecisionTreeMetadata$.buildMetadata(DecisionTreeMetadata.scala:125)\n\tat org.apache.spark.ml.tree.impl.GradientBoostedTrees$.boost(GradientBoostedTrees.scala:333)\n\tat org.apache.spark.ml.tree.impl.GradientBoostedTrees$.run(GradientBoostedTrees.scala:61)\n\tat org.apache.spark.ml.classification.GBTClassifier.$anonfun$train$1(GBTClassifier.scala:210)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n\tat org.apache.spark.ml.classification.GBTClassifier.train(GBTClassifier.scala:171)\n\tat org.apache.spark.ml.classification.GBTClassifier.train(GBTClassifier.scala:59)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:151)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:115)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.lang.IllegalArgumentException: requirement failed: GBTClassifier was given dataset with invalid label 2.0.  Labels must be in {0,1}; note that GBTClassifier currently only supports binary classification.\n\tat scala.Predef$.require(Predef.scala:281)\n\tat org.apache.spark.ml.classification.GBTClassifier.$anonfun$train$2(GBTClassifier.scala:177)\n\tat org.apache.spark.ml.classification.GBTClassifier.$anonfun$train$2$adapted(GBTClassifier.scala:174)\n\tat org.apache.spark.ml.PredictorParams.$anonfun$extractInstances$2(Predictor.scala:96)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:162)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:160)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1429)\n\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:219)\n\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:219)\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1429)\n\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$2(RDD.scala:1207)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2290)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "# Entrenemos el modelo:\n",
    "gbtc_model = gbtc.fit(train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
